{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# a. BASIC WORD EMBEDDINGS WITH TF-IDF"
      ],
      "metadata": {
        "id": "uOfK-TPfHuRm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0trBKxRHmrg",
        "outputId": "0c031bd3-1eb4-4eee-f343-6165e5aa52c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Feature Names:\n",
            "['as' 'be' 'can' 'captures' 'deep' 'embeddings' 'fun' 'idf' 'importance'\n",
            " 'is' 'learned' 'learning' 'represent' 'tf' 'vectors' 'word' 'words']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.         0.5        0.\n",
            "  0.5        0.         0.         0.5        0.         0.5\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.48546061 0.48546061 0.         0.         0.38274272\n",
            "  0.         0.         0.         0.         0.48546061 0.\n",
            "  0.         0.         0.         0.38274272 0.        ]\n",
            " [0.         0.         0.         0.46516193 0.         0.\n",
            "  0.         0.46516193 0.46516193 0.         0.         0.\n",
            "  0.         0.46516193 0.         0.36673901 0.        ]\n",
            " [0.46516193 0.         0.         0.         0.         0.36673901\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.46516193 0.         0.46516193 0.         0.46516193]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample sentences\n",
        "corpus = [\n",
        "    \"Deep learning is fun\",\n",
        "    \"Word embeddings can be learned\",\n",
        "    \"TF-IDF captures word importance\",\n",
        "    \"Embeddings represent words as vectors\"\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus to get the TF-IDF matrix\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the feature names (unique words from the corpus)\n",
        "features = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert sparse matrix to dense array\n",
        "tfidf_matrix = X.toarray()\n",
        "\n",
        "# Display the feature names\n",
        "print(\"TF-IDF Feature Names:\")\n",
        "print(features)\n",
        "\n",
        "# Display the TF-IDF matrix\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# b. GENERATING WORD EMBEDDINGS USING WORD2VEC AND GLOVE"
      ],
      "metadata": {
        "id": "1YJ77EfQIOUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Uninstall numpy and gensim completely\n",
        "!pip uninstall -y numpy gensim\n",
        "\n",
        "# 2. Install a compatible numpy version first\n",
        "!pip install numpy==1.24.3\n",
        "\n",
        "# 3. Then install gensim fresh (no cache to avoid old builds)\n",
        "!pip install --no-cache-dir gensim\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "9D8oiZiIJDBk",
        "outputId": "1c9c12dd-7b4f-4d0e-a730-d038fed98ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n",
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m275.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m201.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, gensim\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.19.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f0a282c7628d4a49b6aaeaa3e24b8543"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import gensim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"\"\"\n",
        "Natural language processing enables computers to understand human language.\n",
        "Word embeddings capture semantic relationships between words in a vector space.\n",
        "Deep learning techniques such as Word2Vec and GloVe are widely used in NLP\n",
        "applications.\n",
        "This is a sample document for generating word embeddings.\n",
        "Another example document is provided for demonstration purposes.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize sentences\n",
        "sentences = [simple_preprocess(line) for line in sample_text.split(\"\\n\") if line.strip()]\n",
        "print(f\"Sample tokenized sentences:\\n{sentences}\\n\")\n",
        "\n",
        "# Train Word2Vec model\n",
        "print(\"Training Word2Vec model...\")\n",
        "w2v_model = gensim.models.Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=50,\n",
        "    window=3,\n",
        "    min_count=1,\n",
        "    sg=1,\n",
        "    workers=4,\n",
        "    epochs=100\n",
        ")\n",
        "\n",
        "print(\"\\nWord2Vec: Similar words to 'document'\")\n",
        "print(w2v_model.wv.most_similar(\"document\", topn=5))\n",
        "\n",
        "# Download pretrained GloVe embeddings (50D)\n",
        "import os\n",
        "if not os.path.exists(\"glove.6B.50d.txt\"):\n",
        "    import wget\n",
        "    import zipfile\n",
        "\n",
        "    print(\"\\nDownloading GloVe embeddings...\")\n",
        "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    wget.download(url)\n",
        "\n",
        "    print(\"\\nExtracting GloVe embeddings...\")\n",
        "    with zipfile.ZipFile(\"glove.6B.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extract(\"glove.6B.50d.txt\")\n",
        "\n",
        "# Convert GloVe format to word2vec format for gensim\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_input_file = \"glove.6B.50d.txt\"\n",
        "word2vec_output_file = \"glove.6B.50d.word2vec.txt\"\n",
        "if not os.path.exists(word2vec_output_file):\n",
        "    glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "\n",
        "# Load GloVe model\n",
        "glove_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
        "\n",
        "print(\"\\nGloVe: Similar words to 'document'\")\n",
        "if \"document\" in glove_model.key_to_index:\n",
        "    print(glove_model.most_similar(\"document\", topn=5))\n",
        "else:\n",
        "    print(\"Word 'document' not in GloVe vocab\")\n",
        "\n",
        "# Visualization function for embeddings using t-SNE\n",
        "def plot_embeddings(words, vectors, title):\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
        "    reduced = tsne.fit_transform(vectors)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    for i, word in enumerate(words):\n",
        "        plt.scatter(reduced[i, 0], reduced[i, 1])\n",
        "        plt.annotate(word, xy=(reduced[i, 0], reduced[i, 1]), fontsize=9)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Word2Vec visualization (top 10 common words)\n",
        "common_words_w2v = list(w2v_model.wv.key_to_index.keys())[:10]\n",
        "w2v_vectors = np.array([w2v_model.wv[word] for word in common_words_w2v])\n",
        "plot_embeddings(common_words_w2v, w2v_vectors, \"Word2Vec Embeddings (t-SNE)\")\n",
        "\n",
        "# GloVe visualization (only those words present in GloVe vocab)\n",
        "common_words_glove = [w for w in common_words_w2v if w in glove_model.key_to_index]\n",
        "glove_vectors = np.array([glove_model[w] for w in common_words_glove])\n",
        "plot_embeddings(common_words_glove, glove_vectors, \"Pretrained GloVe Embeddings (t-SNE)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "ow-M8V7dIB42",
        "outputId": "b60a22df-1886-4c77-e1c6-9d18aca0aef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1954161762.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# c. SENTENCE EMBEDDINGS WITH UNIVERSAL SENTENCE ENCODER\n"
      ],
      "metadata": {
        "id": "vPpd1ULcKKyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy tensorflow tensorflow_hub --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj4xQFjHKtpN",
        "outputId": "e4267b6c-6ba7-4bc7-d725-0eb95e5d0233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.3 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.19.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow and TensorFlow Hub if not already installed\n",
        "!pip install --quiet tensorflow tensorflow_hub\n",
        "\n",
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n",
        "# Load the Universal Sentence Encoder\n",
        "print(\"Loading Universal Sentence Encoder model...\")\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "print(\"Model loaded!\")\n",
        "\n",
        "# Sample input sentences\n",
        "sentences = [\n",
        "    \"This is a sentence.\",\n",
        "    \"Another example sentence.\",\n",
        "    \"Machine learning is fascinating.\",\n",
        "    \"I love natural language processing.\",\n",
        "    \"The sky is blue today.\"\n",
        "]\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = embed(sentences)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nSentence Embeddings Shape:\", embeddings.shape)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    print(f\"Embedding vector (first 5 values): {embeddings[i][:5].numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZkncXNSI0Ky",
        "outputId": "90f2e77a-f986-44b6-ca6c-076e891d11f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Universal Sentence Encoder model...\n",
            "Model loaded!\n",
            "\n",
            "Sentence Embeddings Shape: (5, 512)\n",
            "\n",
            "Sentence: This is a sentence.\n",
            "Embedding vector (first 5 values): [ 0.02881765 -0.02020016  0.01069627  0.03850532 -0.09253702]\n",
            "\n",
            "Sentence: Another example sentence.\n",
            "Embedding vector (first 5 values): [ 0.03328447  0.01292921 -0.00019189  0.00639367 -0.06787535]\n",
            "\n",
            "Sentence: Machine learning is fascinating.\n",
            "Embedding vector (first 5 values): [ 0.0484621  -0.04914229 -0.06110889 -0.05120424 -0.02304145]\n",
            "\n",
            "Sentence: I love natural language processing.\n",
            "Embedding vector (first 5 values): [ 0.00407052 -0.03478802 -0.00435814  0.03229547  0.00223153]\n",
            "\n",
            "Sentence: The sky is blue today.\n",
            "Embedding vector (first 5 values): [-0.04175051 -0.02714938  0.04868532  0.04477606 -0.01378212]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4geyK5meKfV4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}