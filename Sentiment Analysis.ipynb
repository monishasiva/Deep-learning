{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21053618,"sourceType":"kernelVersion"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch scikit-learn","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UlN_SPTusRil","outputId":"76bdcfbc-cc76-4afe-ee05-652a6f3c94c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"]}],"execution_count":3},{"cell_type":"markdown","source":"#a. Basic Sentiment Analysis using Logistic Regression","metadata":{"id":"bMomrkk_9m8v"}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\ndata = {\n    \"review\": [\n        \"I loved this movie, it was fantastic!\",\n        \"Worst film ever. Waste of time.\",\n        \"Absolutely brilliant acting!\",\n        \"Terrible script and bad direction.\",\n        \"The plot was engaging and exciting.\",\n        \"Horrible movie, I hated it.\",\n        \"An excellent performance by the cast.\",\n        \"Not worth watching, very disappointing.\"\n    ],\n    \"sentiment\": [\"positive\", \"negative\", \"positive\", \"negative\",\n                  \"positive\", \"negative\", \"positive\", \"negative\"]\n}\ndf = pd.DataFrame(data)\n\ndef clean_text(text):\n    return text.lower()\n\ndf['cleaned_text'] = df['review'].apply(clean_text)\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n\nvectorizer = TfidfVectorizer(max_features=500)\nX = vectorizer.fit_transform(df['cleaned_text'])\ny = df['sentiment']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n\nsample_review = \"The movie was boring and slow.\"\nsample_clean = clean_text(sample_review)\nsample_vector = vectorizer.transform([sample_clean])\nsample_pred = model.predict(sample_vector)[0]\n\nprint(\"\\nSample Review:\", sample_review)\nprint(\"Predicted Sentiment:\", \"Positive (1)\" if sample_pred == 1 else \"Negative (0)\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMzurjam9Jw9","outputId":"2f964bfc-3393-4f13-f040-c87a67a5d379"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.00\n","\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       2.0\n","           1       0.00      0.00      0.00       0.0\n","\n","    accuracy                           0.00       2.0\n","   macro avg       0.00      0.00      0.00       2.0\n","weighted avg       0.00      0.00      0.00       2.0\n","\n","\n","Sample Review: The movie was boring and slow.\n","Predicted Sentiment: Positive (1)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"execution_count":9},{"cell_type":"markdown","source":"# b. Twitter Sentiment Analysis using LSTM and GloVe Embeddings","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport os\ndf = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv', \n                 encoding='latin1', header=None,\n                 names=['sentiment', 'id', 'date', 'flag', 'user', 'text'])\ndf.head()\ndf = df.sample(100000, random_state=42)\n\n# Clean tweets\ndef clean_tweet(text):\n    text = str(text).lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # URLs\n    text = re.sub(r'\\@\\w+|\\#', '', text)  # Mentions/hashtags\n    text = re.sub(r'[^\\w\\s]', '', text)  # Punctuation\n    return text\n\ndf['cleaned_text'] = df['text'].apply(clean_tweet)\ndf['sentiment'] = df['sentiment'].replace({0:0, 2:1})\n# Check unique sentiment values\nprint(\"Unique sentiment values:\", df['sentiment'].unique())\n# Tokenization\nmax_len = 50\nmax_words = 20000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(df['cleaned_text'])\nsequences = tokenizer.texts_to_sequences(df['cleaned_text'])\nX = pad_sequences(sequences, maxlen=max_len)\ny = to_categorical(df['sentiment'])\n# Verify number of classes\nnum_classes = y.shape[1]\nprint(f\"Number of classes: {num_classes}\")\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Load GloVe embeddings (pre-uploaded to Kaggle)\nembeddings_index = {}\nglove_path = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt'\n\nwith open(glove_path) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, 'f', sep=' ')\n        embeddings_index[word] = coefs\n# Prepare embedding matrix\nembedding_dim = 100\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n# Build LSTM model - NOW WITH 2 OUTPUT CLASSES\nmodel = Sequential([\n    Embedding(max_words, embedding_dim, \n              embeddings_initializer=Constant(embedding_matrix),\n              input_length=max_len, trainable=False),\n    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n    Dense(num_classes, activation='softmax')  # Dynamic based on num_classes\n])\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n# Train model\nhistory = model.fit(X_train, y_train,\n                    batch_size=128,\n                    epochs=5,\n                    validation_split=0.1)\n# Evaluate with correct target names\ny_pred = np.argmax(model.predict(X_test), axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n# Use labels parameter matching your actual classes\nprint(classification_report(y_true, y_pred, \n                           target_names=['Negative', 'Positive'],  # Only 2 classes\n                           labels=[0, 1]))  # Explicitly specify your label values\ndef predict_sentiment(text):\n    text = clean_tweet(text)\n    seq = tokenizer.texts_to_sequences([text])\n    padded = pad_sequences(seq, maxlen=max_len)\n    pred = model.predict(padded)\n    return 'Positive' if np.argmax(pred) == 1 else 'Negative'# Test\ntest_tweets = [\n    \"I love this product!\",\n    \"This is okay I guess\",\n    \"Terrible experience, never buying again\"\n]\n\nfor tweet in test_tweets:\n    print(f\"Tweet: {tweet}\")\n    print(f\"Sentiment: {predict_sentiment(tweet)}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#c. Movie Reviews Sentiment Classification with BERT","metadata":{"id":"sX781pM79yn-"}},{"cell_type":"code","source":"import pandas as pd, torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\ndata = {\n    \"review\": [\n        \"I loved this movie, it was fantastic!\",\n        \"The film was terrible and boring.\",\n        \"What a great experience, highly recommend.\",\n        \"Worst acting I have ever seen.\",\n        \"An absolute masterpiece, brilliant!\",\n        \"I did not enjoy the film at all.\"\n    ],\n    \"sentiment\": [\"positive\",\"negative\",\"positive\",\"negative\",\"positive\",\"negative\"]\n}\ndf = pd.DataFrame(data)\ndf[\"sentiment\"] = df[\"sentiment\"].map({\"negative\":0, \"positive\":1})\n\ntrain_texts, val_texts, y_train, y_val = train_test_split(\n    df[\"review\"], df[\"sentiment\"], test_size=0.2, random_state=42\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\ntok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n\ndef encode(texts, labels):\n    enc = tok(list(texts), truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n    return TensorDataset(enc[\"input_ids\"], enc[\"attention_mask\"], torch.tensor(labels.values))\n\ntrain_loader = DataLoader(encode(train_texts, y_train), batch_size=2, shuffle=True)\nval_loader   = DataLoader(encode(val_texts, y_val), batch_size=2)\n\nopt = AdamW(model.parameters(), lr=2e-5)\nfor epoch in range(2):\n    model.train()\n    for b in train_loader:\n        b = [t.to(device) for t in b]; opt.zero_grad()\n        loss = model(input_ids=b[0], attention_mask=b[1], labels=b[2]).loss\n        loss.backward(); opt.step()\n    print(f\"Epoch {epoch+1} done\")\n\nmodel.eval(); preds, true = [], []\nfor b in val_loader:\n    b = [t.to(device) for t in b]\n    with torch.no_grad(): out = model(b[0], attention_mask=b[1])\n    preds += out.logits.argmax(1).cpu().tolist(); true += b[2].cpu().tolist()\n\nprint(classification_report(true, preds, target_names=[\"Neg\",\"Pos\"]))\n\ndef predict(txt):\n    enc = tok(txt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64).to(device)\n    return \"Positive\" if model(**enc).logits.argmax().item() else \"Negative\"\n\nprint(predict(\"The movie was boring and uninteresting.\"))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jBw9lF0qsUtX","outputId":"8c5d61aa-fca0-4ce3-cae5-284927743e9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 done\n","Epoch 2 done\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","         Neg       0.50      1.00      0.67         1\n","         Pos       0.00      0.00      0.00         1\n","\n","    accuracy                           0.50         2\n","   macro avg       0.25      0.50      0.33         2\n","weighted avg       0.25      0.50      0.33         2\n","\n","Negative\n"]}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"id":"y3Pw3M2E7kBw"},"outputs":[],"execution_count":null}]}